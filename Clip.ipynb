{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da58f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b8e03",
   "metadata": {},
   "source": [
    "Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a1052a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, width, max_seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        # Creating positional encoding\n",
    "        pe = torch.zeros(max_seq_length, width)\n",
    "\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(width):\n",
    "                if i % 2 == 0:\n",
    "                    pe[pos][i] = np.sin(pos/(10000 ** (i/width)))\n",
    "\n",
    "                else: \n",
    "                    pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/width)))\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to embeddings\n",
    "        x = x + self.pe\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89119d76",
   "metadata": {},
   "source": [
    "Build Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2458110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, width, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "\n",
    "    self.query = nn.Linear(width, head_size)\n",
    "    self.key = nn.Linear(width, head_size)\n",
    "    self.value = nn.Linear(width, head_size)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    # Obtaining Queries, Keys, and Values\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "\n",
    "    # Dot Product of Queries and Keys\n",
    "    attention = Q @ K.transpose(-2,-1)\n",
    "\n",
    "    # Scaling\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "\n",
    "    # Applying Attention Mask\n",
    "    if mask is not None:\n",
    "        attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "    attention = attention @ V\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7eb407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, width, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = width // n_heads\n",
    "\n",
    "    self.W_o = nn.Linear(width, width)\n",
    "\n",
    "    self.heads = nn.ModuleList([AttentionHead(width, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    # Combine attention heads\n",
    "    out = torch.cat([head(x, mask=mask) for head in self.heads], dim=-1)\n",
    "\n",
    "    out = self.W_o(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2cc5c",
   "metadata": {},
   "source": [
    "Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66b4fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, width, n_heads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Sub-Layer 1 Normalization\n",
    "        self.ln1 = nn.LayerNorm(width)\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        self.mha = MultiHeadAttention(width, n_heads)\n",
    "\n",
    "        # Sub-Layer 2 Normalization\n",
    "        self.ln2 = nn.LayerNorm(width)\n",
    "\n",
    "        # Multilayer Perception\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.width, self.width*r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.width*r_mlp, self.width)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Residual Connection After Sub-Layer 1\n",
    "        x = x + self.mha(self.ln1(x), mask=mask)\n",
    "\n",
    "        # Residual Connection After Sub-Layer 2\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362d594f",
   "metadata": {},
   "source": [
    "Define a simple tokenzier using chr()\n",
    "- chr(0) -> NULL\n",
    "- chr(2) -> SOT\n",
    "- chr(3) -> EOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13b38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, encode=True, mask=None, max_seq_length=32):\n",
    "    if encode:\n",
    "        out = chr(2) + text + chr(3)\n",
    "        out = out + \"\".join([chr(0) for _ in range(max_seq_length-len(out))]) # Adding padding\n",
    "        out = torch.IntTensor(list(out.encode(\"utf-8\")))\n",
    "        mask = torch.ones(len(out.nonzero()))\n",
    "        mask = torch.cat((mask, torch.zeros(max_seq_length-len(mask)))).type(torch.IntTensor)\n",
    "\n",
    "    else: \n",
    "        out = [chr(x) for x in text[1:len(mask.nonzero())-1]]\n",
    "        out = \"\".join(out)\n",
    "        mask = None\n",
    "\n",
    "    return out, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640fcc4a",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "884105be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,  72, 101, 108, 108, 111,   3,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, mask = tokenizer(\"Hello\", encode=True, max_seq_length=16)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64668847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f064a63",
   "metadata": {},
   "source": [
    "- Text Encoder and Image Encoder (Using Text Transformer and Vision Transformer)\n",
    "- We also can use ResNet or CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c56bf856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, width, max_seq_length, n_heads, n_layers, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length  # Maximum length of input sequence\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, width) # Embedding Table\n",
    "\n",
    "        self.positional_embedding = PositionalEmbedding(width, max_seq_length)\n",
    "\n",
    "        self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])\n",
    "\n",
    "        # learned proj of image to embed\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "    def forward(self, text, mask=None):\n",
    "        # Text Embedding\n",
    "        x = self.encoder_embedding(text)\n",
    "\n",
    "        # Positional Embedding\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x, mask=mask)\n",
    "\n",
    "        # Takes features from the EOT Embedding\n",
    "        x = x[torch.arange(text.shape[0]),torch.sub(torch.sum(mask[:,0],dim=1),1)]\n",
    "\n",
    "        # joint multimodal embedding\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae32afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, width, img_size, patch_size, n_channels, n_layers, n_heads, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "        assert width % n_heads == 0, \"width must be divisible by n_heads\"\n",
    "\n",
    "        self.n_patches = (img_size[0] * img_size[1]) // (patch_size[0] * patch_size[1])\n",
    "\n",
    "        self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.linear_project = nn.Conv2d(n_channels, width, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # Classification Token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, width))\n",
    "\n",
    "        self.positional_embedding = PositionalEmbedding(width,self.max_seq_length)\n",
    "\n",
    "        self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])\n",
    "\n",
    "        # learned proj of image to embed\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Patch Embedding\n",
    "        x = self.linear_project(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Positional Embedding\n",
    "        x = torch.cat((self.cls_token.expand(x.size()[0], -1, -1),x), dim=1)\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x)\n",
    "\n",
    "        # Takes Class Tokens\n",
    "        x = x[:, 0, :]\n",
    "\n",
    "        # joint multimodal embedding\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38166125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_encoder = ImageEncoder(vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, emb_dim)\n",
    "\n",
    "        self.text_encoder = TextEncoder(vocab_size, text_width, max_seq_length, text_heads, text_layers, emb_dim)\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    def forward(self,image,text, mask=None):\n",
    "        I_e = self.image_encoder(image)\n",
    "        T_e = self.text_encoder(text, mask=mask)\n",
    "\n",
    "        # scaled pairwise cosine similarities [n, n]\n",
    "        logits = (I_e @ T_e.transpose(-2,-1)) * torch.exp(self.temperature)\n",
    "\n",
    "        # symmetric loss function\n",
    "        labels = torch.arange(logits.shape[0]).to(self.device)\n",
    "\n",
    "        loss_i = nn.functional.cross_entropy(logits.transpose(-2,-1), labels)\n",
    "        loss_t = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2126a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        super().__init__()\n",
    "        self.dataset = load_dataset('fashion_mnist')\n",
    "        self.transform = T.ToTensor()\n",
    "        \n",
    "        if train:\n",
    "            self.split = \"train\"\n",
    "\n",
    "        else: \n",
    "            self.split = \"test\"\n",
    "\n",
    "        self.captions = {0: \"An image of a t-shirt/top\",\n",
    "                        1: \"An image of trousers\",\n",
    "                        2: \"An image of a pullover\",\n",
    "                        3: \"An image of a dress\",\n",
    "                        4: \"An image of a coat\",\n",
    "                        5: \"An image of a sandal\",\n",
    "                        6: \"An image of a shirt\",\n",
    "                        7: \"An image of a sneaker\",\n",
    "                        8: \"An image of a bag\",\n",
    "                        9: \"An image of an ankle boot\"}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows[self.split]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset[self.split][i]['image']\n",
    "        img = self.transform(img)\n",
    "\n",
    "        cap, mask = tokenizer(self.captions[self.dataset[self.split][i]['label']])\n",
    "\n",
    "        mask = mask.repeat(len(mask), 1)\n",
    "\n",
    "        return {'image': img, 'caption': cap, 'mask': mask}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4a894",
   "metadata": {},
   "source": [
    "Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "004407a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 32\n",
    "vit_width = 9\n",
    "img_size = (28,28)\n",
    "patch_size = (14,14)\n",
    "n_channels = 1\n",
    "vit_layers = 3\n",
    "vit_heads = 3\n",
    "vocab_size = 256\n",
    "text_width = 32\n",
    "max_seq_length = 32\n",
    "text_heads = 8\n",
    "text_layers = 4\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a77038e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FashionMNIST(train = True)\n",
    "test_set = FashionMNIST(train = False)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c41e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 32])\n",
      "tensor([  2,  65, 110,  32, 105, 109,  97, 103, 101,  32, 111, 102,  32,  97,\n",
      "        110,  32,  97, 110, 107, 108, 101,  32,  98, 111, 111, 116,   3,   0,\n",
      "          0,   0,   0,   0], dtype=torch.int32)\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0][\"image\"].shape)\n",
    "print(train_set[0][\"caption\"].shape)\n",
    "print(train_set[0][\"mask\"].shape)\n",
    "print(train_set[0][\"caption\"])\n",
    "print(train_set[0][\"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2448a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 4060 Laptop GPU)\n",
      "Epoch [1/10], Batch Loss: 2.854\n",
      "Model Saved.\n",
      "Epoch [2/10], Batch Loss: 2.766\n",
      "Model Saved.\n",
      "Epoch [3/10], Batch Loss: 2.820\n",
      "Epoch [4/10], Batch Loss: 2.866\n",
      "Epoch [5/10], Batch Loss: 2.743\n",
      "Model Saved.\n",
      "Epoch [6/10], Batch Loss: 2.676\n",
      "Model Saved.\n",
      "Epoch [7/10], Batch Loss: 2.706\n",
      "Epoch [8/10], Batch Loss: 2.721\n",
      "Epoch [9/10], Batch Loss: 2.730\n",
      "Epoch [10/10], Batch Loss: 2.721\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        img, cap, mask = data[\"image\"].to(device), data[\"caption\"].to(device), data[\"mask\"].to(device)\n",
    "        loss = model(img,cap,mask)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Batch Loss: {loss.item():.3f}\")\n",
    "\n",
    "    # Saves model if it performed better than the previous best\n",
    "    if loss.item() <= best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), \"clip.pt\")\n",
    "        print(\"Model Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
